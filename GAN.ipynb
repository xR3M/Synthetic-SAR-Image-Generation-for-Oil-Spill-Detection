{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "    \n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU,  Dense, Flatten, Reshape, Dropout, UpSampling2D, Lambda\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster Information:\n",
      "Number of Bands: 4\n",
      "Width: 256\n",
      "Height: 256\n",
      "Coordinate Reference System (CRS): None\n",
      "Transform (Affine): | 1.00, 0.00, 0.00|\n",
      "| 0.00, 1.00, 0.00|\n",
      "| 0.00, 0.00, 1.00|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\remne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rasterio\\__init__.py:319: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"images_downscaled\\\\April_7_2022.tif\"\n",
    "dataset = rasterio.open(file_path)\n",
    "\n",
    "# Print general information\n",
    "print(\"Raster Information:\")\n",
    "print(f\"Number of Bands: {dataset.count}\")\n",
    "print(f\"Width: {dataset.width}\")\n",
    "print(f\"Height: {dataset.height}\")\n",
    "print(f\"Coordinate Reference System (CRS): {dataset.crs}\")\n",
    "print(f\"Transform (Affine): {dataset.transform}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the Images to a (256, 256) Dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"images\"\n",
    "output_folder = \"images_downscaled\"\n",
    "target_resolution = (256, 256)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over the files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # Check if the file is an image (you can modify the condition based on your image file extensions)\n",
    "    if file_name.endswith(\".tif\"):\n",
    "        # Construct the full paths to the input and output images\n",
    "        input_image_path = os.path.join(input_folder, file_name)\n",
    "        output_image_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "        # Open and resize the input image\n",
    "        input_image = Image.open(input_image_path)\n",
    "        resized_image = input_image.resize(target_resolution)\n",
    "\n",
    "        # Save the resized image to the output folder\n",
    "        resized_image.save(output_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the Masks to a (256, 256) Dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"masks\"\n",
    "output_folder = \"masks_downscaled\"\n",
    "target_resolution = (256, 256)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over the files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    # Check if the file is an image (you can modify the condition based on your image file extensions)\n",
    "    if file_name.endswith(\".tif\"):\n",
    "        # Construct the full paths to the input and output images\n",
    "        input_image_path = os.path.join(input_folder, file_name)\n",
    "        output_image_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "        # Open and resize the input image\n",
    "        input_image = Image.open(input_image_path)\n",
    "        resized_image = input_image.resize(target_resolution)\n",
    "\n",
    "        # Save the resized image to the output folder\n",
    "        resized_image.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 4096)              413696    \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 4096)              0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 128, 128, 1)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 128, 128, 64)      640       \n",
      "                                                                 \n",
      " leaky_re_lu_28 (LeakyReLU)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSampling  (None, 256, 256, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " leaky_re_lu_29 (LeakyReLU)  (None, 256, 256, 64)      0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 256, 256, 64)      0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 256, 256, 64)      0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 256, 256, 4)       2308      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 527,428\n",
      "Trainable params: 527,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# def build_generator():\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Dense(256*256, input_dim = 100))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(Reshape((256, 256, 1))) # 1 is the number of channels\n",
    "\n",
    "#     # Upsampling Block 1\n",
    "#     model.add(UpSampling2D())\n",
    "#     model.add(Conv2D(64, 3, padding='same'))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "\n",
    "#     # Upsampling Block 2\n",
    "#     model.add(UpSampling2D())\n",
    "#     model.add(Conv2D(64, 3, padding='same'))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "\n",
    "#     # Convolutional Block 1\n",
    "#     model.add(Conv2D(64, 2, padding='same'))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "\n",
    "#     # Convolutuonal Block 2\n",
    "#     model.add(Conv2D(64, 2, padding ='same'))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "\n",
    "#     # Convolutional layer to get to one channel\n",
    "#     model.add(Conv2D(1, 1, padding='same', activation='sigmoid'))\n",
    "\n",
    "\n",
    "#     return model\n",
    "\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(64 * 64 * 1, input_dim=100))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((64, 64, 1)))\n",
    "\n",
    "    # Upsampling Block 1\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, 3, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Upsampling Block 2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, 3, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Convolutional Block 1\n",
    "    model.add(Conv2D(64, 3, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Convolutional Block 2\n",
    "    model.add(Conv2D(64, 3, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # # Convolutional layer to get to one channel\n",
    "    # model.add(Conv2D(1, 3, padding='same', activation='sigmoid'))\n",
    "\n",
    "    # Convolutional layer to get to four channels\n",
    "    model.add(Conv2D(4, 3, padding='same', activation='sigmoid'))\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "generator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_discriminator():\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Convolutional Block 1\n",
    "#     model.add(Conv2D(64, (3, 3), strides=(2, 2), input_shape=(256, 256, 1)))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     # Convolutional Block 2\n",
    "#     model.add(Conv2D(128, (3, 3)))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     # Convolutional Block 3\n",
    "#     model.add(Conv2D(256, (3, 3)))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     # Convolutional Block 4\n",
    "#     model.add(Conv2D(512, (3, 3)))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     # Flatten and then Condense into 1\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Block 1\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), input_shape=(256, 256, 1)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Convolutional Block 2\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Convolutional Block 3\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Convolutional Block 4\n",
    "    model.add(Conv2D(512, (3, 3)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Flatten and then Condense into 1\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_36 (Conv2D)          (None, 127, 127, 64)      640       \n",
      "                                                                 \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 127, 127, 64)      0         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 127, 127, 64)      0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 125, 125, 128)     0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 125, 125, 128)     0         \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 123, 123, 256)     295168    \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 123, 123, 256)     0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 123, 123, 256)     0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 121, 121, 512)     1180160   \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 121, 121, 512)     0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 121, 121, 512)     0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 7496192)           0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 7496192)           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 7496193   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,046,017\n",
      "Trainable params: 9,046,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image and mask files\n",
    "image_folder = \"images_downscaled\"\n",
    "mask_folder = \"masks_downscaled\"\n",
    "image_files = os.listdir(image_folder)\n",
    "mask_files = os.listdir(mask_folder)\n",
    "\n",
    "# Set the number of training iterations\n",
    "epochs = 100\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_images(epoch):\n",
    "    # Generate fake images\n",
    "    noise = np.random.normal(0, 1, (10, 100))\n",
    "    fake_images = generator.predict(noise)\n",
    "\n",
    "    # Create a directory to save the generated images\n",
    "    save_dir = \"generated_images\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the generated images\n",
    "    for i, image in enumerate(fake_images):\n",
    "        filename = f\"generated_image_{epoch}_{i}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        Image.fromarray(image).save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_8' (type Sequential).\n    \n    Input 0 of layer \"conv2d_36\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (16, 256, 256, 4)\n    \n    Call arguments received by layer 'sequential_8' (type Sequential):\n      • inputs=tf.Tensor(shape=(16, 256, 256, 4), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39mones((batch_size, \u001b[39m1\u001b[39m)), np\u001b[39m.\u001b[39mzeros((batch_size, \u001b[39m1\u001b[39m))))\n\u001b[0;32m     33\u001b[0m \u001b[39m# Train the discriminator\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m discriminator_loss \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch(x, y)\n\u001b[0;32m     36\u001b[0m \u001b[39m# ---------------------\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Train the generator\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# ---------------------\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# Generate a new batch of noise\u001b[39;00m\n\u001b[0;32m     40\u001b[0m noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (batch_size, \u001b[39m100\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2508\u001b[0m     )\n\u001b[0;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9mrshbxp.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1270\u001b[0m     outputs,\n\u001b[0;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1050\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m-> 1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py:280\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    275\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mvalue\n\u001b[0;32m    276\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape_as_list[\u001b[39mint\u001b[39m(axis)] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\n\u001b[0;32m    277\u001b[0m             value,\n\u001b[0;32m    278\u001b[0m             \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         }:\n\u001b[1;32m--> 280\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    282\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: expected axis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof input shape to have value \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbut received input with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m             )\n\u001b[0;32m    287\u001b[0m \u001b[39m# Check shape.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\remne\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_8' (type Sequential).\n    \n    Input 0 of layer \"conv2d_36\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (16, 256, 256, 4)\n    \n    Call arguments received by layer 'sequential_8' (type Sequential):\n      • inputs=tf.Tensor(shape=(16, 256, 256, 4), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # ---------------------\n",
    "    # Train the discriminator\n",
    "    # ---------------------\n",
    "    # Select a random batch of images and masks\n",
    "    idx = np.random.randint(0, len(image_files), batch_size)\n",
    "    selected_image_files = [image_files[i] for i in idx]\n",
    "    selected_mask_files = [mask_files[i] for i in idx]\n",
    "\n",
    "    # Load and preprocess the real images and masks\n",
    "    real_images = []\n",
    "    real_masks = []\n",
    "    for image_file, mask_file in zip(selected_image_files, selected_mask_files):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        mask_path = os.path.join(mask_folder, mask_file)\n",
    "        image = np.array(Image.open(image_path))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        real_images.append(image)\n",
    "        real_masks.append(mask)\n",
    "    real_images = np.array(real_images)\n",
    "    real_masks = np.array(real_masks)\n",
    "\n",
    "    # Generate fake images\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    fake_images = generator.predict(noise)\n",
    "\n",
    "    # Create a combined batch of real and fake images\n",
    "    x = np.concatenate((real_images, fake_images))\n",
    "    # Create labels for the discriminator (1 for real images, 0 for fake images)\n",
    "    y = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))))\n",
    "\n",
    "    # Train the discriminator\n",
    "    discriminator_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "    # ---------------------\n",
    "    # Train the generator\n",
    "    # ---------------------\n",
    "    # Generate a new batch of noise\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    # Create labels for the generator (1 for all generated images, as we want the discriminator to classify them as real)\n",
    "    y = np.ones((batch_size, 1))\n",
    "\n",
    "    # Train the generator\n",
    "    generator_loss = combined_model.train_on_batch(noise, y)\n",
    "\n",
    "    # Print the losses\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Discriminator Loss: {discriminator_loss} - Generator Loss: {generator_loss}\")\n",
    "\n",
    "    # Save generated images at specified intervals\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_generated_images(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
